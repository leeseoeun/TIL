# 3장 성능을 좌우하는 DB 설계와 쿼리
## 성능의 핵심인 DB

<br>

## 조회 트래픽을 고려한 인덱스 설계
### 단일 인덱스와 복합 인덱스

<br>

### 선택도를 고려한 인덱스 칼럼 선택
왜 선택도가 낮은 `status` 컬럼인데도 인덱스를 추가해야 하는지 아래와 같이 설명드릴게요.

<br>

**✅ 핵심 요약**<br>
`status` 컬럼은 선택도가 낮지만, 자주 조건절(`WHERE`)에 사용되므로 인덱스를 걸 필요가 있다.

<br>

**🔍 배경 정리**<br>
- `status` 컬럼의 값은 `'W'`, `'P'`, `'C'` 세 가지뿐이므로 선택도가 낮습니다.<br>
(예: 고유값 3개 / 전체 행 수 = 낮은 선택도)
- 일반적으로 선택도가 낮은 컬럼은 인덱스 효율이 떨어져서 인덱스를 안 거는 게 일반적입니다.

<br>

**❗️ 하지만 이 경우는 예외예요**<br>
- 실제로 쿼리를 보면 아래와 같이 `status = 'W'` 조건이 지속적이고 반복적으로 사용됩니다:
    ```sql
    select * from jobqueue
    where status = 'W'
    order by jobid asc;
    ```
- 이런 쿼리는 작업 큐에서 대기 중인 작업을 지속적으로 조회하는 용도이기 때문에,<br>
매우 자주 실행되며 응답 속도가 중요합니다.

<br>

**✅ 그래서 인덱스를 거는 이유는?**<br>
1. 선택도는 낮지만 사용 빈도는 높기 때문
- 쿼리가 반복적으로 사용되며 `status`로 필터링하므로,<br>
→ 전체 테이블을 매번 스캔하는 것보다 인덱스를 사용하는 것이 더 효율적입니다.
2. 풀 스캔(Full Table Scan)을 피하기 위해
- `status`에 인덱스가 없으면 DB는 모든 행을 읽어야 해서 느립니다.
- 인덱스가 있으면 해당 값을 빠르게 찾아낼 수 있어 쿼리 성능 향상.

<br>

**🔎 결론**<br>
선택도가 낮더라도, 조건절에서 자주 사용되는 컬럼이라면 인덱스를 걸어주는 것이 바람직합니다.<br>
이건 선택도보다 사용 패턴(쿼리 사용 빈도)이 더 중요할 수 있다는 대표적인 예시입니다.

<br>

### 커버링 인덱스 활용하기
이 페이지는 커버링 인덱스(Covering Index) 개념을 설명하고 있으며, 쿼리 성능을 높이기 위해 인덱스에 어떤 컬럼을 포함해야 하는지에 대해 알려주고 있어요. 하나씩 정리해드릴게요.

<br>

**✅ 커버링 인덱스란?**<br>
정의:
> 커버링 인덱스는 쿼리 실행 시 필요한 모든 컬럼이 인덱스에 포함되어 있어, 실제 테이블(데이터 블록)에 접근할 필요 없이 인덱스만으로 결과를 반환할 수 있는 인덱스를 말해요.

<br>

**📌 설명 요약**<br>
예제 1:
```sql
select * from activityLog
where activityDate = '2024-07-31' and activityType = 'VISIT';
```
- 이 쿼리는 `activityDate`와 `activityType`을 조건으로 사용함.
- 이 두 컬럼이 포함된 인덱스를 사용하면 필터링은 인덱스로, 데이터 조회는 테이블에서 따로 읽어야 해요.
- 이 경우는 인덱스를 통해 대상을 찾은 뒤, 실제 테이블에서 * 을 가져오는 방식 (즉, 테이블 접근이 발생함)
<br>

예제 2: 커버링 인덱스 활용
```sql
select activityDate, activityType
from activityLog
where activityDate = '2024-07-31' and activityType = 'VISIT';
```
- 여기서는 조회 대상도 `activityDate`, `activityType` → 모두 인덱스에 있는 컬럼만 조회
- 이 경우, 테이블에 접근할 필요 없이 인덱스만으로 결과 반환 가능<br>
→ 즉, 커버링 인덱스가 되며 성능이 훨씬 좋아짐

<br>

**✅ 핵심 요약**<br>
| 비교 항목 | 인덱스 + 테이블 접근 (일반 인덱스) | 커버링 인덱스 |
| --- | --- | --- |
| 조건 컬럼 | 인덱스에 있음 | 인덱스에 있음 |
| 조회 컬럼 | 테이블에서 읽어야 함 | 인덱스에 모두 포함 |
| 성능 | 보통 | 빠름 (테이블 접근 없음) |

<br>

**🔍 아래 쿼리와의 연결 ("인덱스는 필요한 만큼만 만들기")**<br>
예제:
```sql
select * from activityLog
where userId = 123 and activityDate = '2024-07-31'
order by activityDatetime desc;
```
이런 쿼리들을 자주 쓴다면:<br>
👉 복합 인덱스를 아래처럼 생성하는 것이 좋음:
```sql
create index idx_log on activityLog(userId, activityDate, activityDatetime);
```
그러면,
- 조건절(`WHERE`)에 사용되는 `userId`, `activityDate`로 빠르게 필터링
- 정렬(`ORDER BY activityDatetime`)까지 인덱스로 처리 가능
- 커버링 인덱스가 되면 테이블 접근도 안 해도 됨

<br>

**✅ 결론**<br>
- 커버링 인덱스를 사용하면 쿼리 성능이 비약적으로 향상됨
- 조회 대상 컬럼까지 인덱스에 포함되도록 설계하는 것이 중요
- 단, 모든 컬럼을 다 인덱스에 넣는 건 오히려 오버헤드가 크므로, 자주 사용하는 쿼리 패턴에 맞춰 딱 필요한 만큼만 포함해야 함

<br>

### 인덱스는 필요한 만큼만 만들기
이 페이지는 "인덱스는 필요한 만큼만 만들기"에 대한 주제를 다루고 있으며, 잘못된 인덱스 설계로 인해 성능이 오히려 나빠질 수 있다는 것을 설명하고 있어요. 아래에 핵심을 정리해서 설명드릴게요.

<br>

**✅ 핵심 주제: 인덱스는 많이 만든다고 좋은 것이 아니다!**<br>
📌 예제 쿼리
```sql
select * from reservation
where name = '예약자 이름'
order by regDt desc;
```
- 이 쿼리는 이름(name)으로 예약자를 찾고, 최근 등록일(regDt 기준)로 정렬하는 쿼리입니다.

<br>

**🔍 설명 요약**<br>
1. 복합 인덱스의 활용
앞에서 언급된 두 쿼리를 빠르게 처리하려고 아래와 같은 복합 인덱스를 만들 수 있다고 했어요:
- `(userId, activityDate)`
- `(userId, activityDate, activityType)`<br>
→ 각각의 쿼리에 딱 맞게 성능을 높여줄 수 있지만…
2. 하지만 인덱스는 무조건 많이 만들면 안 된다!
왜냐하면:
- 읽기 속도는 빨라지지만, 쓰기·수정·삭제 작업이 느려짐
- 인덱스는 데이터 변경 시마다 함께 갱신되기 때문
- 인덱스 자체가 디스크 공간과 메모리를 차지
- 너무 많은 인덱스는 쿼리 최적화기의 선택을 어렵게 하여 오히려 성능 저하를 유발할 수 있음

<br>

**✅ "필요한 만큼만 만들기" 예시**<br>
예를 들어 아래와 같이 단순하게 `reserveDate`에만 인덱스를 걸어놨다고 합시다:
```sql
create index idxRdate on reservation(reserveDate);
```
하지만 실제로는 아래 쿼리를 많이 사용한다면:
```sql
select * from reservation
where name = '예약자 이름'
order by regDt desc;
```
👉 name, regDt에 대한 인덱스가 필요함<br>
그러므로:
```sql
create index idx_name_regdt on reservation(name, regDt desc);
```
이런 식의 인덱스를 만들어줘야 성능이 향상됨. (커버링 인덱스 + 정렬 처리)

<br>

**✅ 결론 정리**<br>
| 잘못된 인덱스 사용 | 올바른 인덱스 설계 기준 |
| --- | --- |
| 자주 안 쓰는 컬럼에도 무조건 인덱스를 건다 | 자주 사용되는 쿼리 기준으로 만든다 |
| 모든 조합을 인덱스로 만든다 | 필요한 조합만 선택해 최소화한다 |
| 읽기만 생각하고 인덱스 늘린다 | 쓰기/갱신 비용도 고려한다 |
> 📌 요점: “인덱스는 성능 향상을 위한 도구지만, 잘못 쓰면 오히려 독이 될 수 있다.”

<br>

* * *

<br>

> "두 번째 인덱스가 효과를 발휘하려면 한 사용자가 하루에 만들어 내는 데이터 개수가 조회 성능에 영향을 줄 만큼 많아야 한다."

<br>

이 말을 이해하려면 인덱스의 선택도(Selectivity)와 필터링 강도, 그리고 인덱스 범위 스캔의 효율성 개념을 함께 이해해야 해요.

<br>

**📌 왜 두 번째 인덱스는 데이터 개수와 관련이 있을까?**<br>
예시: 두 개의 인덱스
1. `(userId, activityDate)`
2. `(userId, activityDate, activityType)`

<br>

이때 쿼리는 다음과 같아요:
```sql
-- 첫 번째 쿼리
select * from activityLog
where userId = 123 and activityDate = '2024-07-31';

-- 두 번째 쿼리
select * from activityLog
where userId = 123 and activityDate = '2024-07-31' and activityType = 'BUY';
```
✅ 첫 번째 인덱스<br>
이 인덱스는 이미 `userId`와 `activityDate`로 "많은 양의 데이터를 효과적으로 줄여줍니다".
> 예를 들어 한 사용자가 하루에 10~20건 로그를 남긴다면, 이미 이 인덱스로 10~20건만 읽으면 되는 것이므로 충분히 빠릅니다.

<br>

그래서 굳이 여기에 `activityType`을 추가하지 않아도 읽을 데이터 양이 적고, 쿼리 성능은 충분히 좋습니다.<br>
✅ 두 번째 인덱스<br>
`activityType`까지 포함한 인덱스를 추가로 만들면 더 좁은 범위로 조회할 수 있지만, 조건으로 줄일 데이터가 이미 매우 적은 경우에는 의미가 없습니다.<br>
예:
- userId=123, activityDate='2024-07-31' → 10건
- 여기서 activityType='BUY' → 3건만 걸림
> 10건에서 3건 걸러내는 데 인덱스가 꼭 필요할까요? 아닙니다.<br>
→ 그냥 메모리에서 필터링해도 충분히 빠릅니다. (비용 대비 효과가 없음)

<br>

즉, 데이터 수가 적을 때는 인덱스를 추가해도 성능 차이를 체감하기 어렵다는 뜻이에요.

<br>

**🔍 왜 첫 번째 인덱스는 데이터 개수 기준 얘기를 하지 않았을까?**<br>
그건 첫 번째 인덱스는 효과가 ‘거의 항상’ 있기 때문이에요.
- userId와 activityDate는 필터로 많은 양의 데이터를 줄여주는 대표적인 조건
- 예를 들어 로그 테이블에 수천만 건이 있어도,
    - userId = 123 AND activityDate = '2024-07-31' 만으로 범위를 수천에서 수십 건으로 줄일 수 있음
> 이건 거의 모든 상황에서 인덱스 효과가 보장되기 때문에 굳이 "데이터 개수가 많아야 의미 있다"는 전제가 필요 없어요.

<br>

**✅ 정리**<br>
| 항목 | 설명 |
| --- | --- |
| 첫 번째 인덱스 `(userId, activityDate)` | 대부분의 경우 데이터 양이 많아 필터링 효과가 크기 때문에 성능 향상이 확실함 |
| 두 번째 인덱스 `(userId, activityDate, activityType)` | 이미 좁아진 결과에서 추가 필터링이므로, 데이터 양이 적으면 성능 차이 없음 |
| 왜 조건 붙음? | 인덱스가 성능에 영향을 줄 만큼 읽을 데이터 양이 충분히 많아야 하기 때문 |

<br>

* * *

<br>

"왜 첫 번째 인덱스 `(userId, activityDate)`는 데이터 개수 조건이 없이도 인덱스 효과가 ‘거의 항상’ 보장되는가?"에 대한 핵심 이유는 바로:<br>
✅ 선택도(Selectivity)가 높은 조합이기 때문이에요.

<br>

**🔍 왜 (userId, activityDate) 조합은 효과가 클까?**<br>
1. userId는 보통 데이터 분포를 잘 나눌 수 있음 (카디널리티가 높음)<br>
→ 사용자 수가 많기 때문에 userId = 123 조건만으로도 전체 테이블의 대부분 데이터를 제외할 수 있어요.<br>
예를 들어:
    - activityLog 테이블에 1000만 건이 있다 해도
    - 특정 사용자(userId = 123)의 로그는 수천 건 정도만 존재할 가능성이 높아요
2. activityDate는 시간 기반 필터로 범위를 좁혀줌<br>
→ 로그성 데이터에서 날짜 조건은 추가적인 강력한 필터입니다.
- 한 사용자의 로그라도 1년 치면 수천 건이지만
- 그 중 하루(activityDate = '2024-07-31')는 수십 건일 수 있음
<br>

즉, 두 조건을 조합하면 매우 좁은 범위로 필터링됩니다.
> userId = 123 → 수천 건<br>
userId = 123 AND activityDate = '2024-07-31' → 수십 건<br>
👉 인덱스 사용 시 훨씬 빠르게 조회 가능

<br>

**🧠 그래서 굳이 "데이터 양이 많아야 인덱스 효과가 있다"는 단서를 붙이지 않아도 되는 이유:**<br>
- 이 조합은 대부분의 테이블에서 항상 필터링 효과가 크기 때문
- 즉, 인덱스를 타고 들어가면 바로 적은 수의 레코드만 읽으면 되니까 인덱스 효과가 체감될 정도로 명확하게 나타납니다

<br>

**⚖️ 반면, 두 번째 인덱스 `(userId, activityDate, activityType)`는 왜 조건을 붙일까?**<br>
- 이미 `(userId, activityDate)` 조건만으로 충분히 좁아진 결과에서
- `activityType = 'BUY'`로 필터링하는 것이기 때문에<br>
→ 추가 인덱스의 효과가 미미할 수 있음<br>
→ 즉, 추가 성능 향상을 체감하려면 레코드 수가 꽤 많아야만 의미가 있음

<br>

**✅ 요약**<br>
| 구분 | 설명 |
| --- | --- |
| `(userId, activityDate)` 인덱스 | 항상 필터링 효과가 큼, 거의 모든 상황에서 인덱스 사용 효과가 있음 |
| `(userId, activityDate, activityType)` 인덱스 | 이미 좁은 범위에서 필터링이므로, 데이터가 많지 않으면 추가 효과가 미미함|
| 결론 | 첫 번째 인덱스는 데이터 수와 무관하게 효과가 큼 → 조건 설명이 생략된 것 |

<br>

## 몇 가지 조회 성능 개선 방법
### 미리 집계하기
이 페이지는 동시성 문제(concurrency issue)에 대해 다루고 있어요. 특히 다음 쿼리에 대해 여러 클라이언트가 동시에 실행했을 때 어떤 일이 벌어질 수 있는가를 설명합니다.

<br>

**🔎 문제 쿼리**<br>
```sql
update survey set answerCnt = answerCnt + 1 where surveyId = 1;
```

<br>

**📌 핵심 질문**<br>
> "이 쿼리를 동시에 5개의 클라이언트가 실행하면 어떻게 될까?"

<br>

**✅ 이상적인 경우 (원자적 atomic 연산일 경우)**<br>
- 모든 update가 충돌 없이 차례대로 실행됨
- `answerCnt`는 정확히 5만큼 증가함<br>
(예: 기존 값이 100이면 → 105)

<br>

**⚠️ 문제가 발생할 수 있는 경우 (원자적이지 않을 경우)**<br>
동시성 문제는 이런 식으로 발생할 수 있어요:
1. 트랜잭션 A: `answerCnt`를 읽음 → 현재 100
2. 트랜잭션 B: 거의 동시에 `answerCnt`를 읽음 → 역시 100
3. 두 트랜잭션 모두 `answerCnt + 1`을 계산해서 → 101로 설정
4. 결과적으로 2개의 update가 실행됐지만 실제 증가량은 1

<br>

💥 즉, 덮어쓰기가 발생하며 정확한 증가량이 보장되지 않음

<br>

**🧠 왜 이런 일이 벌어질까?**<br>
이 쿼리는 다음 두 단계를 포함해요:
1. answerCnt 값을 읽고
2. 그 값에 1을 더한 후, 다시 저장

<br>

이게 DB 내부에서 두 개의 동작으로 분리되어 있기 때문에,
동시에 실행되면 중간에 다른 트랜잭션이 끼어들 수 있어요.

<br>

**💡 해결 방법**<br>
이 문제를 해결하려면 DBMS가 이 연산을 "원자적(atomic)"으로 보장해야 합니다. 방법은 다음과 같아요:
| 방법 | 설명 |
| --- | --- |
| ✅ 트랜잭션 격리 수준 조정 | SERIALIZABLE 같은 높은 격리 수준으로 설정 |
| ✅ 행 수준 Locking | 해당 `surveyId` 행을 수정할 때 Lock을 걸어 중복 접근 방지 |
| ✅ DB의 내부 함수 사용 | 예: `UPDATE ... SET answerCnt = answerCnt + 1`은 일부 DB에서 원자적으로 보장 |
| ✅ `FOR UPDATE` 사용 | 트랜잭션에서 `SELECT ... FOR UPDATE`로 락을 명시적으로 걸기 |

<br>

**✅ 요약**<br>
| 항목 | 설명 |
| --- | --- |
| 문제 | 여러 클라이언트가 동시에 `answerCnt + 1` 실행 시 값이 정확히 증가하지 않을 수 있음 |
| 원인 | `읽기 → 증가 → 저장` 이 분리된 연산이라, 중간에 충돌 발생 가능 |
| 해결 | DB가 원자적 연산으로 처리해야 함. 트랜잭션 격리 수준, 락, 내장 연산자 등을 활용해야 함 |

<br>

* * *

<br>

"트랜잭션 격리 수준 조정 — SERIALIZABLE 같은 높은 격리 수준으로 설정"이 왜 동시성 문제를 해결할 수 있는지 자세히 설명드릴게요.

<br>

**✅ 트랜잭션 격리 수준(Isolation Level)이란?**<br>
트랜잭션 격리 수준은 여러 트랜잭션이 동시에 실행될 때, 서로의 작업을 얼마나 격리시킬 것인지를 결정하는 기준입니다.<br>
SQL 표준에는 다음과 같은 네 가지 격리 수준이 있어요:
| 격리 수준 | 현상 방지 | 동시 처리 성능 |
| --- | --- | --- |
| READ UNCOMMITTED | 아무것도 방지하지 않음 | 가장 빠름 |
| READ COMMITTED | Dirty Read 방지 | 빠름 |
| REPEATABLE READ | Dirty, Non-repeatable Read 방지 | 중간 |
| SERIALIZABLE | Dirty, Non-repeatable, Phantom 방지 | 가장 느림 |

<br>

**🔒 SERIALIZABLE이란?**<br>
`SERIALIZABLE`은 가장 엄격한 트랜잭션 격리 수준입니다.
- 마치 모든 트랜잭션이 순차적으로(직렬로) 실행된 것처럼 보이도록 보장합니다.
- 트랜잭션 간에 읽기/쓰기 충돌이 일어나지 않도록 락을 강하게 걸어 처리합니다.

<br>

**🔁 동시성 문제 해결 예시**<br>
아까 예로 든 쿼리:
```sql
update survey set answerCnt = answerCnt + 1 where surveyId = 1;
```
이 쿼리를 5개 클라이언트가 동시에 실행할 때:
- `SERIALIZABLE` 격리 수준에서는 하나의 트랜잭션이 끝나기 전에는 다른 트랜잭션이 해당 `surveyId`를 읽지도, 쓰지도 못하게 막아줍니다.
- 즉, 모든 트랜잭션이 순서대로 실행되는 효과를 보장해 주므로, 결과적으로 `answerCnt`는 정확하게 5만큼 증가하게 됩니다.

<br>

**⚠️ 주의점**<br>
- `SERIALIZABLE`은 성능 비용이 큽니다.
    - 동시에 많은 트랜잭션이 들어오면 락 대기가 많아지고, 처리 속도가 느려질 수 있습니다.
    - 특히 다수의 `update` 쿼리를 처리할 때 병목이 생길 수 있어요.

<br>

**✅ 대안**<br>
- 꼭 `SERIALIZABLE`을 쓰지 않더라도 DBMS에 따라 `answerCnt + 1` 연산 자체를 원자적으로 처리해 주는 경우도 많습니다.
    - 예: MySQL의 InnoDB에서는 `update table set count = count + 1`이 내부적으로 원자적 처리됨
- 또는 비관적 락(Pessimistic Lock) 또는 낙관적 락(Optimistic Lock) 방식도 고려할 수 있습니다.

<br>

**📌 요약**<br>
| 항목 | 설명 |
| --- | --- |
| 개념 | 트랜잭션 간의 간섭을 최소화하는 수준 설정 |
| SERIALIZABLE | 모든 트랜잭션이 순서대로 실행되는 것처럼 동작 |
| 장점 | 동시성 문제 (값 충돌, 오염된 읽기 등) 완벽하게 방지 |
| 단점 | 성능 저하 가능, 락 경합 증가 |
| 대안 | DBMS의 원자적 연산, 락(Lock) 활용 등 |

<br>

* * *

<br>

트랜잭션 격리 수준에서 자주 등장하는 개념인 Dirty Read, Non-repeatable Read, Phantom Read에 대해 각각 쉽게 설명드릴게요.

<br>

**✅ 1. Dirty Read (더러운 읽기)**<br>
> 다른 트랜잭션이 커밋하지 않은 값을 읽어버리는 현상

<br>

예시:
- 트랜잭션 A: `UPDATE account SET balance = balance - 100 WHERE id = 1;` (아직 커밋 X)
- 트랜잭션 B: `SELECT balance FROM account WHERE id = 1;` → 변경된 값 읽음

<br>

→ 그런데 트랜잭션 A가 롤백하면?<br>
👉 트랜잭션 B는 존재하지 않는 값을 읽은 꼴이 됨.

<br>

방지 방법:
- 격리 수준을 READ COMMITTED 이상으로 설정

<br>

**✅ 2. Non-repeatable Read (반복 불가능한 읽기)**<br>
> 같은 쿼리를 두 번 실행했는데, 결과가 달라지는 현상

<br>

예시:
- 트랜잭션 A: `SELECT balance FROM account WHERE id = 1;` → 결과: 1000
- 트랜잭션 B: `UPDATE account SET balance = 500 WHERE id = 1;` → 커밋
- 트랜잭션 A: `SELECT balance FROM account WHERE id = 1;` → 결과: 500

<br>

→ 트랜잭션 A 입장에서는 같은 조건으로 조회했는데 값이 달라져서 "읽은 값을 믿을 수 없음" 상태가 됨

<br>

방지 방법:
- 격리 수준을 REPEATABLE READ 이상으로 설정

<br>

**✅ 3. Phantom Read (팬텀 리드)**<br>
> 같은 조건으로 여러 번 쿼리했는데, 행의 개수가 달라지는 현상

<br>

예시:
- 트랜잭션 A: `SELECT * FROM orders WHERE amount > 1000;` → 5건
- 트랜잭션 B: `INSERT INTO orders (amount) VALUES (1500);` → 커밋
- 트랜잭션 A: 다시 `SELECT * FROM orders WHERE amount > 1000;` → 6건

<br>

→ 트랜잭션 A 입장에서는 처음에는 없던 "팬텀 행"이 갑자기 생김

<br>

방지 방법:
- 격리 수준을 SERIALIZABLE로 설정

<br>

**🧩 요약 비교표**<br>
| 현상 | 발생 조건 | 방지 격리 수준 | 비고 |
| --- | --- | --- | --- |
| Dirty Read | 커밋 전 데이터 읽음 | READ COMMITTED 이상 | 존재하지 않는 데이터 읽을 수 있음 |
| Non-repeatable Read | 같은 조건에 결과 달라짐 | REPEATABLE READ 이상 | UPDATE/DELETE에 의한 변화 |
| Phantom Read | 같은 조건에 행 수 달라짐 | SERIALIZABLE | INSERT에 의한 변화 |

<br>

### 페이지 기준 목록 조회 대신 ID 기준 목록 조회 방식 사용하기
추가 데이터 존재 여부를 판단하는 로직을 SQL 쿼리 결과 기반으로 서버에서 처리하는 방식을 설명한 것입니다.

<br>

**✅ 기본 쿼리**
```sql
SELECT * 
FROM article 
WHERE id < 1001 AND deleted = false 
ORDER BY id DESC 
LIMIT 11;
```

<br>

**✅ 서버 측 로직 (예: Java)**
```java
List<Article> results = jdbcTemplate.query(query); // 또는 JPA, MyBatis 등 사용

boolean hasNext = results.size() > 10;

List<Article> responseData = results.size() > 10 
    ? results.subList(0, 10) 
    : results;
```

<br>

**✅ 응답 예시**<br>
```json
{
  "articles": [ ... 최대 10개 ... ],
  "hasNext": true  // 또는 false
}
```

<br>

**🧠 로직 요약**<br>
1. LIMIT 11 로 요청보다 하나 더 많이 조회함
2. 결과가 11개면:
    - 10개만 사용자에게 보내고
    - `hasNext = true`
3. 결과가 10개 이하이면:
    - 전부 보내고
    - `hasNext = false`

<br>

**✅ 이 방식의 장점**<br>
- 별도의 `COUNT(*)` 쿼리 없이 빠르게 "더 가져올 데이터 있는지" 판단
- 무한스크롤, 커서 기반 페이지네이션에 최적화

<br>

### 조회 범위를 시간 기준으로 제한하기
**✅ 1. DB가 사용하는 메모리 캐시란?**<br>
DBMS는 성능을 높이기 위해 디스크에서 자주 접근하는 데이터를 메모리에 미리 올려두는 캐시 공간을 운영합니다. 대표적인 예는 다음과 같습니다:<br>
🔸 InnoDB Buffer Pool (MySQL InnoDB 엔진)
- 디스크에서 읽어온 페이지(데이터/인덱스)를 메모리에 캐싱해 둠
- 같은 쿼리가 또 오면 디스크를 읽지 않고 메모리에서 바로 제공 → 성능 향상

<br>

🔸 PostgreSQL의 Shared Buffer
- PostgreSQL도 자주 사용되는 테이블/인덱스 페이지를 메모리에 올려둠

<br>

🔸 Oracle의 SGA (System Global Area)
- DB 블록, SQL 실행 계획, 커서 등 다양한 정보를 메모리에 저장

<br>

**✅ 2. "자동으로 사용되나요?"**<br>
→ 네. DBMS가 자동으로 관리합니다.
- 사용자가 명시적으로 캐시하라고 하지 않아도,
- 쿼리가 자주 발생하면 DB는 해당 데이터를 메모리에 올려두고,
- 다음 쿼리부터는 디스크 대신 메모리에서 처리하게 됩니다.

<br>

**✅ 3. 이 내용에서 말하는 캐시 전략은?**<br>
이 책 내용은 다음을 말합니다:
> "최신 데이터 위주로 조회하도록 기능을 설계하면, 최신 데이터는 자주 조회되므로 DB의 메모리 캐시에 남아 있을 확률이 높아진다. 따라서 성능도 향상된다."

<br>

즉,
- 최신 데이터는 자주 조회됨
- 자주 조회되면 DB는 그 데이터를 캐시에 유지함
- 그러므로, 최신 데이터 위주로 설계하면 캐시 적중률이 높아져서 빠르게 응답할 수 있다는 뜻입니다.

<br>

**✅ 4. 참고로, 애플리케이션 캐시와는 다릅니다**<br>
> Redis, Memcached 같은 외부 캐시를 말하는 건 아님.

<br>

이건 DB 내부 캐시를 뜻하고 있고, Redis 같은 외부 캐시는 애플리케이션에서 별도로 설정/관리하는 것입니다.

<br>

### 전체 개수 세지 않기
이 페이지는 "전체 개수 세지 않기", 즉 `COUNT(*)` 사용을 최소화하자는 성능 최적화 원칙을 설명하고 있습니다. 아래에 내용을 쉽게 정리해 드릴게요.

<br>

**🔹 문제 상황**<br>
회원 목록 조회 기능을 만든다고 가정할 때, 흔히 다음과 같은 두 쿼리를 함께 실행합니다:
```sql
-- 실제 데이터를 조회하는 쿼리
select id, ... 
from member
where name like '지은%'
order by id desc
limit 20;

-- 전체 개수를 세는 쿼리
select count(*)
from member
where name like '지은%';
```

<br>

**🔹 왜 문제가 될까?**<br>
초기에는 문제가 없습니다. 하지만 시간이 지나고 데이터가 많아지면 문제가 커집니다.<br>
문제의 핵심:
- `count(*)`는 조건에 맞는 전체 행을 끝까지 읽어서 세야 합니다.
- 즉, `where name like '지은%'`에 해당하는 모든 행을 읽고 세야 하므로,
- 데이터가 많아질수록 실행 시간이 기하급수적으로 느려짐.

<br>

심지어 인덱스를 타더라도, 커버링 인덱스가 아닌 이상 결국 디스크에서 실제 데이터를 읽어야 합니다.

<br>

**🔹 실제 서비스에서 성능 이슈**<br>
- 리스트 하단에 “전체 몇 명” 같은 문구를 넣으려고 `count(*)`를 반복 호출하면,
- 트래픽이 많아질수록 DB가 느려지고 과부하 발생 가능성.
- 특히 커버링 인덱스가 없거나, 복잡한 조건이 포함된 `LIKE`, `JOIN`, `GROUP BY` 등이 있으면 더욱 심각.

<br>

**✅ 해결 방법 또는 대안**
1. 전체 개수는 표시하지 않는 UI 구성
    - 예: "더 보기" 버튼만 제공하고 전체 개수는 아예 안 보여줌.
2. 백그라운드로 비동기 처리하여 추후에 제공
    - 페이지 렌더링 후 JS로 비동기 요청 → count 값을 느긋하게 보여줌.
3. 대략적인 개수 캐싱
    - 예: 일정 주기로 미리 계산된 카운트를 Redis 등에 저장해 빠르게 제공.
4. 추가 데이터 존재 여부만 판단 (limit + 1 방식)
    - 이미 이전 페이지에서 설명한 방법처럼,
        ```sql
        limit 21
        ```
        로 가져와서 "다음 페이지 있음/없음"만 알려줌.

<br>

**📌 결론**<br>
> 실무에서 `count(*)`는 무조건 쓰면 안 되는 쿼리는 아니지만, 대량 데이터가 걸릴 경우 심각한 병목이 될 수 있기 때문에 반드시 사용 여부를 고민하고 협의해야 한다는 내용을 설명하는 것입니다.

<br>

* * *

> `count(*)`는 조건에 맞는 전체 행을 끝까지 읽어서 세야 합니다.

<br>

이 뜻은 다음과 같습니다:

<br>

**✅ 의미 풀이**<br>
`COUNT(*)`는 단순히 행의 개수만 세는 함수이지만, 그 개수를 세기 위해서는 조건에 맞는 모든 행을 조회해야 합니다.<br>
예를 들어 아래 쿼리:
```sql
SELECT COUNT(*)
FROM member
WHERE name LIKE '지은%';
```
이 쿼리를 실행하면, DB는 다음과 같은 과정을 겪습니다:
1. `member` 테이블 전체에서 `name LIKE '지은%'` 조건에 맞는 모든 행을 탐색합니다.
2. 조건에 맞는 행을 하나하나 세면서 카운트를 증가시킵니다.
3. 마지막 행까지 조건 비교 및 세기를 완료해야만 결과를 반환할 수 있습니다.

<br>

**📌 왜 탐색이 비쌀까?**<br>
- 조건이 단순하더라도 일일이 확인해야 하고,
- 인덱스를 사용해도 인덱스로만 카운트가 불가능한 경우, 결국 실제 테이블의 모든 관련 데이터를 읽어야 하며,
- 그게 수십만, 수백만 행이라면 시간이 오래 걸립니다.

<br>

**✅ 결론**<br>
> 즉, “조건에 해당하는 모든 데이터를 탐색해야 하기 때문에 `COUNT(*)`가 느려진다”는 말은 정확하며, 그로 인해 성능 병목이 생길 수 있다는 것을 경고하는 문장입니다.

<br>

* * *

<br>

**📌 원문:**<br>
> "커버링 인덱스를 사용하더라도 전체 인덱스를 스캔해야 하며, 커버링 인덱스가 아닌 경우에는 실제 데이터를 전부 읽어야 한다."

<br>

**1️⃣ 커버링 인덱스란?**<br>
커버링 인덱스(Covering Index)는 말 그대로, 쿼리 결과에 필요한 모든 컬럼이 인덱스에 포함되어 있어서<br>
DB가 테이블(실제 데이터)까지 내려가지 않고도 인덱스만으로 결과를 반환할 수 있는 인덱스입니다.<br>
예시:
```sql
SELECT name
FROM member
WHERE name LIKE '지은%';
```
➡ `name` 컬럼에 인덱스가 있다면, 커버링 인덱스가 됩니다.<br>
왜냐하면 `WHERE`과 `SELECT` 모두 `name`만을 사용하니까, 테이블까지 접근할 필요 없이 인덱스만으로 결과가 나옵니다.

<br>

**2️⃣ "전체 인덱스를 스캔해야 한다"는 의미**<br>
커버링 인덱스를 사용하더라도, 조건이 `LIKE '지은%'`처럼 범위 조건이면,<br>
해당 조건에 맞는 인덱스 값을 전부 읽어봐야 하므로 인덱스를 처음부터 끝까지 스캔할 수 있습니다.<br>
즉,
- 커버링 인덱스여도 → 범위 조건이 있으면 인덱스 전체 탐색이 발생할 수 있다.
- 그래서 성능 이점은 있지만, 완전히 빠른 것은 아님.

<br>

**3️⃣ "커버링 인덱스가 아닌 경우에는 실제 데이터를 전부 읽어야 한다"**<br>
이건 더 무거운 작업입니다.<br>
예시:
```sql
SELECT id, name, created_at
FROM member
WHERE name LIKE '지은%';
```
➡ 이 경우, name에 인덱스가 있어도 id, created_at이 인덱스에 없으면,<br>
DB는 인덱스로 조건을 확인한 다음 해당 행을 실제 테이블에서 다시 읽어야 합니다.<br>
이걸 "테이블 액세스" 또는 "테이블 루킹(table lookup)"이라고 합니다.<br>
결론적으로:
- ❗ 인덱스만으로 처리할 수 없으니 → 성능이 더 느림
- 특히 `COUNT(*)`처럼 모든 조건 일치 행을 전부 세야 할 때 → 모든 행을 실제 테이블에서 읽는 작업이 발생함

<br>

**✅ 요약**<br>
| 상황 | 동작 | 성능 영향 |
| --- | --- | --- |
| 커버링 인덱스 사용 | 인덱스만으로 처리 가능, 하지만 범위 조건이면 전체 인덱스 스캔 | 보통 빠름 |
| 커버링 인덱스 아님 | 인덱스로 조건 확인 후 테이블에서 데이터 다시 읽음 | 느림 |
| `count(*)` 사용 | 조건에 맞는 모든 데이터 탐색 필수 | 데이터 많으면 매우 느림 |

<br>

### 오래된 데이터 삭제 및 분리 보관하기
이 페이지는 DELETE 쿼리와 디스크 사용량, 그리고 단편화(fragmentation)에 대해 설명하고 있습니다. 핵심 요점을 하나씩 풀어드릴게요.

<br>

**✅ 1. DELETE 쿼리를 사용하면 디스크 용량이 줄어들까?**<br>
아니요, 일반적으로는 줄어들지 않습니다.
- DELETE는 데이터를 삭제하는 것이 아니라 '삭제되었다'는 표시만 남기는 것입니다.
- 따라서, 삭제된 공간은 DB 내부적으로는 여전히 차지하고 있고, 나중에 다른 데이터가 들어올 때 재사용됩니다.
> 예: 마치 문서를 지워도 그 빈 칸이 남아 있는 것과 유사

<br>

**✅ 2. 단편화(Fragmentation)란?**<br>
데이터가 삽입, 수정, 삭제되면서 테이블 내부에 빈 공간이나 조각난 데이터 블록이 생기는 현상입니다.
- 새 데이터를 추가하면 빈 공간에 채워 넣지만,
- 이 빈 공간이 중간중간 흩어져 있으면 효율적으로 저장되지 않음
- 이게 쌓이면 디스크가 파편처럼 흩어져 있어 성능 저하 발생

<br>

**✅ 3. 단편화가 심해지면?**<br>
- 디스크 I/O 증가: 데이터를 읽으려면 여기저기 흩어진 블록을 따라가야 하므로 비용이 커짐
- 디스크 낭비: 실제 저장되는 데이터보다 더 많은 공간을 차지하게 됨
- 쿼리 성능 저하: 디스크 접근 속도, 캐시 효율 등에 영향을 줌

<br>

**✅ 4. 해결 방법: 최적화(OPTIMIZE)**
- DBMS는 보통 단편화를 줄이기 위한 '최적화 명령'을 제공합니다.
    - 예: MySQL의 `OPTIMIZE TABLE` 명령
- 이 과정에서 데이터를 재배치하고 공간을 정리해 성능을 높이며, 실제 디스크 사용량도 줄일 수 있습니다.

<br>

**📌 요약**<br>
| 항목 | 설명 |
| --- | --- |
| DELETE | 디스크 공간을 줄이지 않음. 삭제 표시만 남김 |
| 단편화 | 삽입/삭제가 반복되며 생긴 빈 공간들이 DB 성능을 저하시킴 |
| 영향 | I/O 증가, 성능 저하, 디스크 공간 낭비 |
| 해결책 |최적화 작업을 통해 데이터 재배치 및 공간 회수 |

<br>

* * *

<br>

Oracle에서는 MySQL처럼 단순하게 `OPTIMIZE TABLE`을 사용할 수는 없지만, 단편화를 줄이고 공간을 회수하는 작업은 다음과 같은 방법으로 수행합니다:

<br>

**✅ Oracle에서 단편화(공간 낭비) 해결 방법**<br>
1. `ALTER TABLE ... MOVE`
- 데이터를 새로운 블록에 재배치해서 단편화를 줄이고 공간을 정리함.
- 단점: 인덱스가 무효화되므로 다시 리빌드해야 함
```sql
ALTER TABLE 테이블명 MOVE;
-- 예: ALTER TABLE employees MOVE;
```
> ※ 실행 후 관련된 인덱스는 아래처럼 다시 빌드해야 합니다:
```sql
ALTER INDEX 인덱스명 REBUILD;
```

<br>

2. `SHRINK SPACE` (ASSM 테이블스페이스에서만 가능)
- Automatic Segment Space Management (ASSM)가 적용된 테이블스페이스에서 사용 가능
- 데이터를 재배치하면서 동시에 인덱스도 자동으로 유지
- 온라인에서 실행 가능 (서비스 중단 없음)
```sql
ALTER TABLE 테이블명 ENABLE ROW MOVEMENT;
ALTER TABLE 테이블명 SHRINK SPACE;
```
> ※ `ROW MOVEMENT`를 켜야 Oracle이 레코드를 자유롭게 이동시킬 수 있음

<br>

3. `COALESCE` (인덱스 단편화 해소용)
- 인덱스의 조각난 블록들을 병합해서 공간을 정리
```sql
ALTER INDEX 인덱스명 COALESCE;
```
> `REBUILD`보다 I/O가 적게 발생하고, 실시간 적용이 가능한 장점이 있음

<br>

**🔄 요약**<br>
| 작업 | 명령어 | 특징 |
| --- | --- | --- |
| 테이블 이동 | `ALTER TABLE ... MOVE` | 단편화 제거, 인덱스 재빌드 필요 |
| 온라인 정리 | `SHRINK SPACE` | 인덱스 무효화 없음, 온라인 가능 |
| 인덱스 정리 | `ALTER INDEX ... COALESCE` | 빠르고 가벼운 인덱스 공간 정리 |
| 인덱스 재생성 | `ALTER INDEX ... REBUILD` | 가장 강력한 인덱스 단편화 제거 |

<br>

어떤 명령어가 "가장 나은지"는 운영 환경, 시스템 부하, 다운타임 허용 여부, 단편화 수준, 인덱스 재빌드 비용 등에 따라 달라지지만, 아래 기준을 참고해 선택하시면 됩니다:

<br>

**✅ 운영 중(서비스 중단 불가)이고, 단편화가 심하지 않다면**<br>
👉 `SHRINK SPACE`
- 장점:
    - 온라인 수행 가능 (서비스 중단 X)
    - 인덱스 자동 유지 (재빌드 필요 없음)
- 조건: ASSM(Automatic Segment Space Management) 방식의 테이블스페이스에서만 가능
- 단점:
    - 성능이 아주 빠르진 않음
    - 일부 환경에서는 `MOVE`보다 정리 효과가 덜함

<br>

**✅ 강력한 정리가 필요하고, 인덱스 재빌드 가능하다면**<br>
👉 `ALTER TABLE ... MOVE` + `ALTER INDEX ... REBUILD`
- 장점:
    - 단편화 제거 효과가 가장 큼
    - 공간 회수 및 성능 최적화에 확실함
- 단점:
    - 인덱스가 무효화되므로 꼭 리빌드해야 함
    - 일반적으로 운영 중에 수행하기 어려움 (Lock 발생 가능)

<br>

**✅ 인덱스만 단편화되었고, 테이블은 정상이면**<br>
👉 `ALTER INDEX ... COALESCE` 또는 `REBUILD`
- `COALESCE`는 가볍게 온라인으로 수행 가능
- `REBUILD`는 더 강력한 정리이지만 리소스 소모 큼

<br>

**🔍 실무에서의 선택 기준 요약**<br>
| 상황 | 추천 명령어 |
| --- | --- |
| 운영 중이고 다운타임 없이 최적화 | `SHRINK SPACE` |
| 테이블/인덱스 단편화가 매우 심하고, 재시작 가능 | `MOVE` + `REBUILD` |
| 인덱스만 단편화됨 | `COALESCE` 또는 `REBUILD` |
| 테이블스페이스가 ASSM이 아님 | `MOVE` 방식만 가능 |

<br>

### DB 장비 확장하기

<br>

### 별도 캐시 서버 구성하기

<br>

## 알아두면 좋을 몇 가지 주의 사항
### 쿼리 타임아웃

<br>

### 상태 변경 기능은 복제 DB에서 조회하지 않기
아래 문장은 DB 복제 환경에서의 데이터 정합성 문제를 설명하는 핵심적인 내용입니다:
> "주 DB와 복제 DB 간 데이터 복제는 트랜잭션 커밋 시점에 이뤄진다. 주 DB의 트랜잭션 범위 내에서 데이터를 변경하고, 복제 DB에서 변경 대상이 될 수 있는 데이터를 조회하면 데이터 불일치로 인해 문제가 생긴다."

<br>

**🔷 쉽게 설명하면?**<br>
1. 주 DB는 데이터를 직접 변경하는 곳이고,
2. 복제 DB는 읽기 전용으로 사용하는 보조 데이터베이스입니다.
3. 주 DB에서 데이터 변경은 트랜잭션이 커밋될 때 복제 DB에 반영됩니다.

<br>

**🔷 실제 시나리오 예시**<br>
1. 어떤 트랜잭션이 주 DB에서 회원 정보를 다음과 같이 변경하고 있습니다.
    ```sql
    -- 트랜잭션 시작
    BEGIN;
    UPDATE member SET status = '정지' WHERE id = 123;
    -- 아직 COMMIT하지 않음
    ```
2. 그런데 이 트랜잭션이 커밋되기 전에, 다른 서비스나 백엔드가 복제 DB에서 id=123 회원의 정보를 조회합니다:
    ```sql
    -- 복제 DB에서 조회
    SELECT status FROM member WHERE id = 123;
    ```
3. 이 경우, 복제 DB에는 아직 `status = '정지'`가 반영되지 않았기 때문에,<br>
기존 값(예: '정상')이 조회됩니다.

<br>

**🔶 그래서 왜 문제가 되나요?**<br>
- 트랜잭션이 아직 커밋되지 않았기 때문에 복제 DB는 옛날 데이터를 보여주게 됩니다.
- 반면 주 DB에서는 데이터가 이미 변경 중입니다.
- 이로 인해 동일한 시점에 DB를 조회하더라도, 사용하는 위치에 따라 다른 결과가 나올 수 있음.

<br>

➡️ 이것이 데이터 불일치(Inconsistency) 문제입니다.

<br>

**🔷 예를 들어, 이런 문제들이 생길 수 있어요**<br>
| 상황 | 설명 |
| --- | --- |
| 회원가입 직후 이메일 중복 확인 | 주 DB에 insert되었지만 아직 커밋 전이라 복제 DB에 없음 → "없는 이메일"로 잘못 판단 |
| 결제 상태 변경 직후 상태 조회 | 결제는 성공했는데 복제 DB에는 아직 반영 안 됨 → "결제 전"으로 잘못 보임 |
| 회원 정지 처리 후 로그인 차단 | 정지 상태가 복제 DB에는 안 보임 → 로그인 허용됨 |

<br>

**🔷 결론: 이 상황에서 지켜야 할 원칙**<br>
✅ 트랜잭션 안에서 변경된 데이터는 무조건 주 DB에서 조회해야 함<br>
❌ 복제 DB에서 조회하면 아직 반영 안 된 "옛날 데이터"를 볼 수 있음<br>
⚠️ 특히 INSERT, UPDATE, DELETE와 연관된 데이터 조회는 주 DB를 사용

<br>

* * *

<br>

문장 "주 DB와 복제 DB 간 데이터 복제는 트랜잭션 커밋 시점에 이뤄진다."는 DB 복제(Replication)의 일관성(consistency)과 타이밍과 관련된 매우 중요한 개념입니다. 아래에 쉽게 설명해드릴게요.

<br>

**🔷 1. 문장의 의미**<br>
- 주(DB): 데이터가 실제로 기록·수정되는 원본 데이터베이스
- 복제(DB): 주 DB의 데이터를 읽기 전용으로 복사해두는 보조 데이터베이스

<br>

이때, 복제 DB로의 데이터 전송(복제)은 주 DB에서 트랜잭션이 커밋된 후에만 이뤄진다는 의미입니다.<br>
즉:
```
BEGIN → 작업 → COMMIT → 복제 DB에 반영
```
> 아직 커밋되지 않은 작업 내용은 복제 DB에 복제되지 않습니다.

<br>

**🔷 2. 왜 커밋 후에 복제하는가?**<br>
트랜잭션의 원자성(Atomicity)과 일관성(Consistency)을 지키기 위해서입니다.
- 트랜잭션이 중간에 롤백(Rollback)될 수도 있기 때문에,
- 커밋 전 데이터를 복제해버리면 복제 DB가 잘못된 데이터를 반영하게 됩니다.

<br>

따라서 트랜잭션이 정상적으로 커밋된 이후에만 해당 데이터를 복제하는 것이 안전합니다.

<br>

**🔷 3. 예시로 설명**<br>
▪️ 상황
1. 주 DB에서 사용자의 이름을 변경하는 트랜잭션을 수행 중
```sql
BEGIN;
UPDATE member SET name = '홍길동' WHERE id = 1;
-- 아직 COMMIT 안 함
```
2. 복제 DB는 아직 이 변경을 모름
3. 주 DB에서 COMMIT이 발생하면, 그제야 복제 DB로 변경 내용이 전파됨

<br>

**🔷 4. 주의해야 할 점**<br>
- 이처럼 커밋 이후 복제가 이뤄지기 때문에, 복제 DB는 항상 약간의 지연(latency)이 있습니다.
- 복제 DB에서 데이터를 조회할 경우, 방금 수정된 데이터가 아직 반영되지 않았을 수 있음.

<br>

**🔷 결론**<br>
✅ 복제 DB는 커밋된 데이터만 반영하므로 정합성이 보장됩니다.<br>
❌ 하지만 그로 인해 실시간성과는 약간의 지연이 생깁니다.<br>
🧠 따라서 중요한 변경 직후의 조회는 반드시 주 DB를 사용해야 합니다.

<br>

### 배치 쿼리 실행 시간 증가

<br>

### 타입이 다른 칼럼 조인 주의
이 페이지는 SQL에서 서로 다른 테이블 컬럼 간의 데이터 타입이 다를 경우 발생할 수 있는 성능 문제에 대해 설명하고 있습니다. 요점 위주로 정리해 드릴게요.

<br>

**🔷 문제 상황 요약**<br>
- 두 테이블이 있습니다:
    - `user` 테이블: `userId`는 `INTEGER`
    - `push` 테이블: `receiverId`는 `VARCHAR`
- 아래처럼 JOIN을 수행하려고 할 때:
    ```sql
    select u.userId, u.name, p.*
    from user u, push p
    where u.userId = p.receiverId
    and p.receiverType = 'MEMBER'
    order by p.id desc
    limit 100;
    ```

<br>

**🔶 어떤 문제가 생기나?**<br>
👉 `userId`는 숫자형(INTEGER), `receiverId`는 문자열형(VARCHAR)입니다.
- 이 두 컬럼을 조인하면 MySQL은 타입이 다른 두 값을 비교해야 합니다.
- MySQL에서는 문자형 기준으로 암묵적 형 변환이 일어납니다.
- 이 형 변환 때문에 인덱스를 제대로 사용할 수 없고, 성능이 급격히 저하될 수 있습니다.

<br>

**🔷 해결 방법**<br>
형 변환을 명시적이고 일관된 방향으로 지정해야 합니다. 예를 들면:
```sql
select u.userId, u.name, p.*
from user u, push p
where cast(u.userId as char character set utf8mb4) collate 'utf8mb4_unicode_ci' = p.receiverId
and p.receiverType = 'MEMBER'
order by p.id desc
limit 100;
```
> 이 방법은 명시적으로 `userId`를 문자형으로 바꿔서 `receiverId`와 비교합니다.
암묵적 형변환보다 훨씬 명확하고, 인덱스를 어느 정도 활용할 수 있는 가능성을 높입니다.

<br>

**🔶 중요한 포인트**<br>
📌 조인 조건에 사용되는 컬럼들의 타입은 꼭 일치시키는 것이 좋습니다.<br>
📌 만약 설계상 타입이 다를 수밖에 없다면, JOIN할 때 명시적으로 형 변환을 지정하세요.<br>
📌 타입 불일치가 성능 저하나 잘못된 결과를 유발할 수 있습니다.

<br>

**✅ 결론**<br>
- 컬럼 타입은 일관되게 유지하고, 조인 대상이라면 특히 신중하게 설계해야 합니다.
- 서로 다른 타입끼리 조인 시에는 명시적 형변환을 사용하고, 가능한 경우 타입을 통일하세요.
- 운영 중 테이블 변경은 성능과 안정성에 큰 영향을 줄 수 있으므로 꼭 충분히 테스트 후 적용해야 합니다.

<br>

* * *

<br>

```sql
-- user 테이블의 userId, name 컬럼과 push 테이블의 모든 컬럼을 조회
select u.userId, u.name, p.* 
from user u, push p 
-- user.userId를 문자형(char)으로 변환한 값과 push.receiverId(문자형)를 비교
-- 문자 집합도 utf8mb4로, 정렬 방식도 'utf8mb4_unicode_ci'로 명시하여 일관성을 유지
where cast(u.userId as char character set utf8mb4) 
      collate 'utf8mb4_unicode_ci' = p.receiverId 
-- 수신 대상(receiverType)이 'MEMBER'인 경우만 필터링
  and p.receiverType = 'MEMBER'
-- push 테이블의 id 기준으로 내림차순 정렬하여 최근 항목이 먼저 오도록 함
order by p.id desc
-- 결과는 최대 100건까지만 조회
limit 100;
```

<br>

**🔍 각 요소의 의미**<br>
| 요소 | 설명 |
| --- | --- |
| `cast(u.userId as char character set utf8mb4)` | `userId`는 정수형이므로 문자형(`CHAR`)으로 변환하여 `receiverId`와 비교 가능하게 만듦 |
| `collate 'utf8mb4_unicode_ci'` | 문자열 비교 시 사용하는 정렬 기준을 `utf8mb4_unicode_ci`로 강제함<br>→ `receiverId`와의 비교 시 정렬 충돌 방지 |
| `p.receiverId` | 문자형(varchar)으로 저장된 값. 외부 시스템이나 사용자에게 전달받은 ID일 수 있음 |
| `p.receiverType = 'MEMBER'` | 수신자 유형 필터링. 예: `'GUEST'`, `'SYSTEM'`, `'MEMBER'` 등 다양한 유형 중 'MEMBER'만 조회 |
| `order by p.id desc` | 최신 push 알림을 먼저 보여주기 위함 |
| `limit 100` | 결과가 너무 많을 경우를 방지해 성능 저하를 막고, 프론트엔드에서 페이지네이션 가능하게 함 |

<br>

**💡 왜 이렇게 작성했는가?**<br>
- MySQL은 비교하는 두 컬럼의 데이터 타입이 다르면 인덱스를 사용할 수 없거나 성능이 저하됨
- `receiverId`에 인덱스가 걸려 있는데, `userId`가 숫자형이므로 그냥 비교하면 암묵적 형변환이 일어나면서 인덱스 무시
- 그래서 `userId`를 명시적으로 문자형으로 변환 → 인덱스를 사용할 가능성 증가

<br>

**✅ 정리**<br>
- 형변환이 필요한 경우, 숫자를 문자로 바꾸는 것이 더 안전
- 명시적 형변환 + 문자 집합 + Collation 지정 → 예상치 못한 비교 오류 방지
- 인덱스 사용 가능성 증가 + 성능 개선

<br>

* * *

<br>

`receiverId`에 설정된 인덱스를 활용하기 위해 `userId`를 명시적으로 문자열(char)로 형변환한 것입니다.

<br>

**✅ 왜 `userId`를 `CHAR`로 변환했는가?**<br>
```sql
cast(u.userId as char character set utf8mb4) collate 'utf8mb4_unicode_ci'
```
- `receiverId`는 `VARCHAR` 타입이고 문자형 인덱스가 걸려 있음
- 그런데 `userId`는 `INTEGER` 타입이므로 데이터 타입이 서로 다르면 MySQL은 인덱스를 비효율적으로 사용하거나 무시
- 따라서 `userId`를 명시적으로 문자형으로 변환하면, `receiverId`와의 비교가 문자열 기준이 되어 `receiverId`에 설정된 인덱스를 사용할 수 있는 여지가 생김

<br>

**🔍 왜 `receiverId`를 숫자로 변환하지 않았는가?**
- `receiverId`는 일반적으로 다양한 문자열이 들어올 수 있기 때문에 숫자로 변환하는 게 위험함
- 예: `'MEMBER'`, `'GUEST'`, `'abc123'` 같은 값이 있다면 숫자 변환 시 오류 또는 의미 없는 값 발생 가능
- 반면 `userId`는 항상 숫자이므로 문자형으로 안전하게 변환 가능

<br>

**🔎 인덱스는 정말 사용되나?**<br>
MySQL에서는 이와 같이 명시적 형변환을 했을 때:
- 조건에 따라 `receiverId` 인덱스가 부분적으로 사용될 수 있음
- 특히 `receiverId`가 앞에 있는 단일 인덱스이거나, 복합 인덱스의 선두 컬럼일 경우 가능성이 높음

<br>

다만 완전한 인덱스 탐색(Index Seek)은 어려울 수 있고, `EXPLAIN`을 통해 확인해 보는 게 가장 정확합니다.

<br>

**✅ 요약**<br>
- `userId`를 문자형으로 변환한 이유: `receiverId` 인덱스를 사용하기 위해서
- 왜 `receiverId`가 아니라 `userId`를 변환했는가: `receiverId`는 문자형이라 숫자 변환이 위험하므로
- 실질적으로 인덱스 사용 여부는 `EXPLAIN`으로 검증 필요

<br>

* * *

<br>

"인덱스를 제대로 사용할 수 없다"는 건, 조인 조건에 사용된 컬럼(userId, receiverId)에 대해 정의된 인덱스가 무력화된다는 뜻입니다.

<br>

**📌 상황 정리**<br>
테이블 구조
- `user.userId` → `INTEGER`, 프라이머리 키 (혹은 인덱스 있음)
- `push.receiverId` → `VARCHAR`, 보통 인덱스 존재

<br>

**✅ 일반적인 경우 (타입이 같고 인덱스 사용 가능)**<br>
```sql
SELECT ...
FROM user u
JOIN push p ON u.userId = p.receiverId
```
- `user.userId` 와 `push.receiverId`의 데이터 타입이 같고, 인덱스가 있다면 → 인덱스 탐색(Index Seek) 이 가능
- 조인이 효율적으로 수행됨

<br>

**❌ 타입이 다를 때 (형 변환 발생)**<br>
```sql
SELECT ...
FROM user u
JOIN push p ON u.userId = p.receiverId
```
- `user.userId`는 `INTEGER`
- `push.receiverId`는 `VARCHAR`
- MySQL은 암묵적으로 `push.receiverId`를 숫자로 바꾸거나 `user.userId`를 문자열로 변환하여 비교
- 이때 인덱스를 사용하지 못하고, 풀 테이블 스캔 또는 풀 인덱스 스캔 발생
> 📉 즉, `push.receiverId`에 인덱스가 있어도 MySQL은 형 변환 후 함수 기반 비교가 되므로 인덱스 무시

<br>

**🔍 어떤 인덱스가 영향을 받는가?**<br>
- `push.receiverId`에 걸린 인덱스 → 조인에서 사용되지 않음
- `user.userId`에 걸린 인덱스도 경우에 따라 비효율적일 수 있음 (MySQL은 조인 순서 및 옵티마이저 판단에 따라 다름)

<br>

**🔑 정리하면**<br>
- 사용 못 하는 인덱스: `receiverId`에 설정된 일반 인덱스 혹은 복합 인덱스
- 이유: 데이터 타입이 달라서 `WHERE`, `JOIN`, `ON` 조건에서 형 변환이 발생하고, 이는 인덱스 탐색(Index Seek)을 방해
- 해결책: 타입을 통일하거나, 명시적으로 형변환을 적용하고 해당 컬럼에 함수 기반 인덱스(Function-based Index)를 고려

<br>

### 테이블 변경은 신중하게

<br>

### DB 최대 연결 개수

<br>

## 실패와 트랜잭션 고려하기
**📘 그림 3.13 설명 – 트랜잭션 누락에 따른 일관성 문제**<br>
✅ 시나리오 흐름 요약
```sql
1. DB 연결
2. SELECT * FROM contract WHERE ...
3. INSERT INTO contract ...
4. UPDATE userStatus SET ...
```
💡 어떤 문제가 있는가?<br>
이 흐름에서는 트랜잭션 처리를 하지 않으면 다음과 같은 문제가 발생할 수 있습니다:
- 사용자의 계약 정보를 처리하면서 `contract` 테이블에 데이터를 넣고,
- 그 결과에 따라 `userStatus` 테이블의 상태도 함께 바꿔야 합니다.

<br>

하지만 트랜잭션으로 묶여 있지 않다면, 예를 들어 3번 INSERT는 성공했지만, 4번 UPDATE가 실패하는 상황이 발생할 수 있습니다.<br>
→ 이렇게 되면 `contract`와 `userStatus` 사이에 데이터 불일치(inconsistency) 가 발생하게 됩니다.<br>
📌 즉, DB에 일부만 반영되고 나머지는 실패하여 비즈니스적으로는 실패지만, 데이터는 일부 반영된 상태가 되어버리는 거죠.

<br>

**💻 `@Transactional` 코드 설명 – 트랜잭션 처리 범위와 예외 처리**<br>
✅ 코드 설명
```java
@Transactional  // 트랜잭션 시작
public void join(JoinRequest join) {
    memberDao.insert(member);  // 1. DB에 데이터 추가

    try {
        mailClient.sendMail(...);  // 2. 메일 전송
    } catch (Exception ex) {
        // 3. 메일 전송 실패해도 예외 무시
        // 4. 로그 기록 등 모니터링만 수행
    }
}
// 트랜잭션 커밋
```
💡 어떤 의미인가?
- 이 코드는 DB 작업은 반드시 성공해야 하고, 메일 전송 실패는 무시해도 되는 경우를 처리합니다.
- `@Transactional`로 DB 작업은 트랜잭션 범위 내에 묶여 있어 중간에 오류 발생 시 롤백됩니다.
- 그러나 `sendMail()`은 try-catch로 감싸져 있어 예외가 발생해도 DB 트랜잭션에는 영향을 주지 않습니다.
- 즉, 메일 전송 실패는 무시하고 로그만 남기며, DB 작업은 그대로 커밋됩니다.

<br>

**🔁 두 내용 연결해서 보면…**<br>
그림 3.13의 문제 해결 방법이 두 번째의 트랜잭션 코드예요.
- 그림 3.13에서는 SELECT → INSERT → UPDATE 작업이 트랜잭션 없이 순차 실행되어, 중간에 실패 시 데이터 일관성이 깨질 수 있었죠.
- 그에 대한 해결책으로, 모든 작업을 하나의 `@Transactional` 트랜잭션으로 묶어야 한다는 내용을 두 번째 코드에서 보여주고 있는 겁니다.
- 외부 시스템 연동(메일 전송)은 트랜잭션 밖에서 실행되도록 처리하여, 서비스 주요 흐름은 중단되지 않도록 설계한 것이죠.

<br>

**✅ 핵심 요약**<br>
| 구분 | 설명 |
| 그림 3.13 | 트랜잭션이 없을 때 발생할 수 있는 데이터 불일치 문제를 설명 |
| 트랜잭션 코드 | 데이터 작업은 트랜잭션으로 묶고, 외부 연동(메일)은 별도로 처리해 안정성 확보 |

<br>

* * *

<br>

```sql
1. DB 연결  
2. SELECT * FROM contract WHERE ...  
3. INSERT INTO contract ...  
4. UPDATE userStatus SET ...
```
여기서 트랜잭션 처리는 다음처럼 해야 데이터 일관성을 보장할 수 있습니다.

<br>

**✅ 트랜잭션 처리 방식**<br>
이 흐름은 `contract` 테이블에 INSERT 하고, 그에 따라 `userStatus`도 UPDATE하는 하나의 논리적 작업 단위입니다. 따라서 전체를 하나의 트랜잭션으로 묶어야 합니다.

<br>

**💡 트랜잭션 처리 예시 (Spring 기준)**<br>
```java
@Service
public class ContractService {

    @Transactional
    public void processContract(Long userId, ContractRequest request) {
        // ① 계약 중복 체크
        Optional<Contract> existing = contractRepository.findByUserIdAndDate(userId, request.getDate());
        if (existing.isPresent()) {
            throw new IllegalStateException("이미 계약된 날짜입니다.");
        }

        // ② 계약 정보 저장
        Contract newContract = new Contract(userId, request.getDate(), request.getDetails());
        contractRepository.save(newContract);

        // ③ 사용자 상태 업데이트
        userStatusRepository.updateStatus(userId, "CONTRACTED");
    }
}
```

<br>

**✅ 핵심 요점**<br>
| 처리 항목 | 설명 |
| `@Transactional` | 메서드 전체를 하나의 트랜잭션으로 묶음 |
| SELECT → INSERT → UPDATE | 모두 성공하면 커밋, 하나라도 실패하면 롤백 |
| 예외 발생 시 | 자동으로 트랜잭션 롤백됨 (`RuntimeException` 기준) |

<br>

**😱 트랜잭션이 없으면 어떤 문제가?**<br>
- `INSERT INTO contract`는 실행되었지만
- `UPDATE userStatus`는 예외로 실패했다면<br>
→ 계약은 존재하지만 상태는 미갱신된 사용자가 생겨 데이터 불일치 발생

<br>

**📌 정리**<br>
| 상황 | 트랜잭션 적용 |
| 단일 논리 작업에 여러 쿼리가 포함된 경우 | 반드시 하나의 트랜잭션으로 묶어야 함 |
| 일부만 성공하고 일부 실패 시 | 전체를 롤백해야 데이터 일관성 유지됨 |

<br>

* * *

<br>

메서드에 `@Transactional`을 붙이면, 그 메서드 내에서 실행되는 모든 DB 작업이 하나의 트랜잭션으로 묶입니다.

<br>

**✅ @Transactional 작동 방식 요약**<br>
예시 코드
```java
@Service
public class UserService {

    @Transactional
    public void registerUser(User user) {
        userRepository.save(user);              // ① INSERT
        userProfileRepository.save(profile);    // ② INSERT
        log.info("사용자 등록 완료");
    }
}
```
👉 동작 흐름
1. `registerUser()`가 호출되면, 스프링 AOP가 트랜잭션을 시작합니다.
2. 내부에서 실행되는 `save()` 등 DB 작업은 같은 트랜잭션 안에서 실행됩니다.
3. 메서드가 정상 종료되면 커밋, 예외가 나면 롤백됩니다.

<br>

**🎯 전제 조건**<br>
1. 스프링이 관리하는 Bean이어야 함 (즉, `@Service`, `@Component` 등으로 등록된 클래스에서 동작함)
2. 프록시 기반이기 때문에 자기 자신 내부 호출에는 적용되지 않음
    ```java
    // ❌ 같은 클래스 내에서의 자기 호출은 트랜잭션 적용 안 됨
    public void outer() {
        inner(); // 트랜잭션 적용 안 됨
    }

    @Transactional
    public void inner() {
        // DB 작업
    }
    ```
    👉 이럴 땐 `TransactionTemplate`이나 `ApplicationContext.getBean()` 방식으로 우회해야 합니다.

<br>

**📝 정리**<br>
| 항목 | 설명 |
| --- | --- |
| 적용 대상 | 메서드 단위 |
| 적용 범위 | 메서드 내 DB 작업 전체 |
| 기본 롤백 대상 | `RuntimeException`, `Error` 발생 시 |
| 성공 시 | 트랜잭션 커밋 |
| 실패 시 | 트랜잭션 롤백 |
